# -*- coding: utf-8 -*-
"""
Created on Thu Nov 30 11:07:54 2017

@author: ptillotson
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Nov  5 18:59:36 2017
@author: ptillotson
"""

## converts training.csv to a dictionary of dictionarys. 
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve
from sklearn.datasets import make_classification
from time import time
from sklearn.model_selection import train_test_split
from sklearn import cross_validation
import matplotlib.pyplot as plt
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.preprocessing import minmax_scale
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import seaborn as sns
import re

train_data = pd.read_csv("train.csv")
#train_data["Survived"] = train_data["Survived"].astype("int")
#train_data["SibSp"] = train_data["SibSp"].astype("int")
#train_data["Parch"] = train_data["Parch"].astype("int")
train_data["Embarked"] = train_data["Embarked"].astype("category")
#print(train_data.head())
    


train_data['Sex'].replace(['male','female'],[0,1],inplace=True)
train_data["Sex"] = train_data["Sex"].astype("int")
train_data["Embarked"].replace(["S", "C", "Q"],[1,2,3],inplace=True)
train_data.loc[(train_data.Embarked.isnull()), "Embarked"]=1
train_data["Embarked"] = train_data["Embarked"].astype("int")
    


train_data.loc[(train_data.Age.isnull()) & (train_data["SibSp"] > 1), 'Age']=9
train_data.loc[(train_data.Age.isnull()) & (train_data["SibSp"] == 1) & (train_data["Parch"] > 0), 'Age']=9
train_data.loc[(train_data.Age.isnull()) & (train_data["SibSp"] <= 1) & (train_data["Parch"] == 2), 'Age']=9
train_data.loc[(train_data.Age.isnull()) & (train_data["SibSp"] <= 1) & (train_data["Parch"] == 1), 'Age']=30
train_data.loc[(train_data.Age.isnull()) & (train_data["SibSp"] <= 1) & (train_data["Parch"] == 0), 'Age']=30
#train_data["Age"] = train_data["Age"].astype("int")
####Top code age to 73 since that is the upper level of the Gaussian distribution
train_data.loc[(train_data["Age"]>=74), 'Age']=73

#Put Fare into quartile buckets since the distribution of values is skewed. 
train_data['Fare']=pd.qcut(train_data.Fare, q=8, labels=False)

##Rare title work such as Mr. and Mrs.
##Names have unique titles such as Mrs or Mr. Let's extract that and place it into a new column.
def get_title(passenger):
    line = passenger
    if re.search('Mrs', line):
        return 'Mrs'
    elif re.search('Mr', line):
        return 'Mr'
    elif re.search('Miss', line):
        return 'Miss'
    elif re.search('Master', line):
        return 'Master'
    else:
        return 'Other'
##Add title column to train_data with title (i.e. Mr or Mrs)  
train_data['Title'] = train_data['Name'].apply(get_title)
##Convert Title to a number representing the Title
train_data.loc[(train_data["Title"]=="Mrs"), 'Title']=0
train_data.loc[(train_data["Title"]=="Mr"), 'Title']=1
train_data.loc[(train_data["Title"]=="Miss"), 'Title']=2
train_data.loc[(train_data["Title"]=="Master"), 'Title']=3
train_data.loc[(train_data["Title"]=="Other"), 'Title']=4

train_data['Family_Size']=0
train_data['Family_Size']=train_data['Parch']+train_data['SibSp']


##Set variable for minmax scaling


train_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Parch', 'Embarked','Pclass'],axis=1,inplace=True)
print(train_data.head(1))
train,test=cross_validation.train_test_split(train_data,test_size=0.3,random_state=42,stratify=train_data['Survived'])
features_train=train[train.columns[1:]]
labels_train=train[train.columns[:1]]
features_test=test[test.columns[1:]]
labels_test=test[test.columns[:1]]
features=train_data[train_data.columns[1:]]
labels=train_data['Survived']

rf = RandomForestClassifier(min_samples_split=10, max_features="sqrt")
#t0 = time()
rf.fit(features_train, labels_train)
#print "training time:", round(time()-t0, 3), "s"
#t0 = time()
pred = rf.predict(features_test)
#print "training time:", round(time()-t0, 3), "s"
accuracy = rf.score(features_test, labels_test)
pred_recall = recall_score(labels_test, pred)
pred_precision = precision_score(labels_test, pred)
print('Feature Importance', rf.feature_importances_)
print("Recall", pred_recall)
print("Precision", pred_precision)
#print pred
print("Accuracy", accuracy)

importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_],
             axis=0)
indices = np.argsort(importances)

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.barh(range(features_train.shape[1]), importances[indices],
       color="r", xerr=std[indices], align="center")
# If you want to define your own labels,
# change indices to a list of labels on the following line.
plt.yticks(range(features_train.shape[1]), indices)
plt.ylim([-1, features_train.shape[1]])
plt.show()

